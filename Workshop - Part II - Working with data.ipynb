{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Data Science and APIs to Support Your Collections Work\n",
    "\n",
    "# Part II: Working with Data\n",
    "\n",
    "Now that we've covered the fundamentals of working within Python, let's get cracking on putting this to work for the purpose of collections assessment. We'll start by compiling a relevant dataset by querying the Scopus API, and then work with that data in the pandas module â€“ purpose built for data analysis.\n",
    "\n",
    "## The plan\n",
    "In this Jupyter Notebook, we will be covering:\n",
    "* [querying the Scopus API to get data, and import it into pandas](#Get-data-from-an-API)\n",
    "* [data analysis and manipulation using pandas](#Data-analysis-and-manipulation-with-pandas)\n",
    "* [use Python to access a library link resolver](#Verify-journal-holdings-with-a-library-link-resolver)\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data from an API\n",
    "\n",
    "### What are APIs?\n",
    "An application programming interface (API), is software that helps applications talk to each other in a machine-friendly way. It acts as a middle point between us and the database that we're trying to get data from. \n",
    "\n",
    "An API is usually publicly available, **web-based** and returns data in a JSON or XML format (machine-readable). The API itself is not the database or the server, but rather a middleman. \n",
    "\n",
    "\n",
    "<img src=\"img/api.png\"/>\n",
    "\n",
    "Source: [Perry Eising (*Medium* article)](https://medium.com/@perrysetgo/what-exactly-is-an-api-69f36968a41f)\n",
    "\n",
    "APIs are great because they allow us to download data programmatically, and not by hand. This lets us automate the work that we would otherwise do by hand in these databases.\n",
    "\n",
    "### Scopus API\n",
    "Scopus is a citation index developed by Elsevier. Its database contains both bibliographic records from a wide-range of journals, but also the citations between these articles.\n",
    "\n",
    "As librarians, we're used to accessing this information through the graphical interface and using the built-in features to access certain types of information by hand. Almost everything that we can do by hand in these databases, we can also do in the API.\n",
    "\n",
    "#### Getting started with the Scopus API\n",
    "Scopus is a proprietary product, and as such, there are some hoops that we must jump through. One, we must access the API while connected to a subscribing institution's internet network (if you're connected to the University of Windsor WiFi, then you're good).\n",
    "\n",
    "For many APIs, we must also [register with the API provider](https://dev.elsevier.com/index.html) in order to generate a unique key (API key) that we will use in the API to authenticate ourselves. For the purpose of this workshop, we will be using a pre-authorized key that I have registered with Elsevier. \n",
    "\n",
    "We read the [API documentation](https://dev.elsevier.com/api_docs.html) in order to see how the API works, what fields we can use and how we must construct our query. \n",
    "\n",
    "#### How does it work?\n",
    "APIs are web-based, meaning that we access the API over HTTP protocol (just like when we are accessing a website on the internet). Requests are made in the form of specially-constructed URLs. The URL needs to contain the API key that we get through registration, and our query in the format required by the API.\n",
    "\n",
    "For example, here is an API request for the Scopus database.\n",
    "\n",
    "[`https://api.elsevier.com/content/search/scopus?query=all(gene)&apiKey=7f59af901d2d86f78a1fd60c1bf9426a`](https://api.elsevier.com/content/search/scopus?query=all(gene)&apiKey=7f59af901d2d86f78a1fd60c1bf9426a)\n",
    "\n",
    "Let's deconstruct this request.\n",
    "\n",
    "> `https://api.elsevier.com/content/search/scopus`: the base URL for the Scopus search API. \n",
    "\n",
    "> `?query=` The start of our query.\n",
    "<br>\n",
    "\n",
    "> `all(gene)`: our search query. We are searching **all** fields for the word \"gene\".\n",
    "<br>\n",
    "\n",
    "> `&`: the `&` symbol is used to combine arguments in our request.\n",
    "<br>\n",
    "\n",
    "> `apiKey=7f59af901d2d86f78a1fd60c1bf9426a`: the API key. In this case, this is the example API key that is provided in the Scopus API documentation.\n",
    "\n",
    "**Try it out!** Click on the API request URL to run it. What happens?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Challenge 5</b> \n",
    "</div>\n",
    "\n",
    "Let's practice querying the Scopus API in-browser using a custom URL and the [Scopus API documentation](https://dev.elsevier.com/tips/ScopusSearchTips.htm). Use all of the parts of the URL request as we described above. including the **base URL for the API**, the **start of the query**, **the search query** itself, the **`&`** symbol between arguments and the **API key**.\n",
    "\n",
    "1. Construct an API request that finds articles with the both of the following keywords: **antimicrobial resistance** and **hospital or hospitals**.\n",
    "2. Take some time to read through the XML output. What is the pattern? What kind of data is returned and how is it structured?\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so we can do this in a web browser? But can we send HTTP requests through Python? Yes! To do so, we will need to install a Python module called **requests**. \n",
    "\n",
    "#### Install the requests module\n",
    "Python comes pre-installed with many useful functions, but we can get more functions by installing **modules** or **packages** created by third-parties. By downloading the Anaconda distribution, we already installed a lot of modules but there are many more out there. \n",
    "\n",
    "We can install modules and packages using PIP, the package manager for Python. PIP is included with the latest version of Python. \n",
    "\n",
    "##### Download modules in macOS & Linux\n",
    "1. Open a new terminal window\n",
    "2. In the terminal, type `pip install requests`\n",
    "\n",
    "##### Download modules in Windows\n",
    "Windows makes this a *bit* more complicated than with Unix systems. We can't simply open the command line to download modules, we need to open the **Anaconda Prompt**.\n",
    "1. Search for **Anaconda Prompt** in the Windows start menu.\n",
    "2. In the Anaconda Prompt, type `pip install requests`\n",
    "    \n",
    "    \n",
    "If you can't find the **Anaconda Prompt** in the Windows start menu:\n",
    "1. Go to the **Anaconda Navigator**\n",
    "2. Go to **Environments**\n",
    "3. Click on the triangle next to **base (root)**\n",
    "4. Select **Open in terminal**\n",
    "5. Type `pip install requests`\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've installed the requests module, we have to load it up in our current Python session. We do this by executing the `import` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests #import requests into this session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create a new variable, and use the requests **get** function. This sends the HTTP request over the internet, just like we did on our own in the browser, but within Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the requests module send our API request, and then save it to the variable 'r'\n",
    "r = requests.get('https://api.elsevier.com/content/search/scopus?query=all(gene)&apiKey=7f59af901d2d86f78a1fd60c1bf9426a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we read the request that we get back by using the requests **text** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.text #read the text from the XML response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doesn't look too good, eh? While all of the data we want is here, executing HTTP requests in Python is a bit labourous. We would need to go through this XML response and parse out all of the relevant data, requiring a lot of coding. This is still a lot less work than doing the searching manually, but this is still time-consuming.\n",
    "\n",
    "Thankfully, other people around the world have developed other modules specifically for the Scopus API to handle the querying and parsing, in order to make it super easy for us! This is where the **pybliometrics** module comes in.\n",
    "\n",
    "### pybliometrics\n",
    "[pybliometrics](https://pybliometrics.readthedocs.io/en/stable/) is a Python 'wrapper' for the Scopus API. It allows us to easily pull data from the Scopus database, and is how we'll be getting our data for this workshop. To get started, we will need to install the pybliometrics module just like we did with requests.\n",
    "\n",
    "##### Install pybliometrics in macOS & Linux\n",
    "1. Open a new terminal window\n",
    "2. In the terminal, type `pip install pybliometrics`\n",
    "\n",
    "##### Install pybliometrics in Windows\n",
    "Windows makes this a *bit* more complicated than with Unix systems. We can't simply open the command line, we to download modules, we need to open the **Anaconda Prompt**.\n",
    "1. Search for **Anaconda Prompt** in the Windows start menu.\n",
    "2. In the Anaconda Prompt, type `pip install pybliometrics`\n",
    "    \n",
    "    \n",
    "If you can't find the **Anaconda Prompt** in the Windows start menu:\n",
    "1. Go to the **Anaconda Navigator**\n",
    "2. Go to **Environments**\n",
    "3. Click on the triangle next to **base (root)**\n",
    "4. Select **Open in terminal**\n",
    "5. Type `pip install pybliometrics`\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that its installed, we need to import pybliometrics into our Python session. The first time you run pybliometrics, you'll be asked to enter in your API key. You only need to do this once and don't need to write it out everytime that you run a request against the API â€“ pybliometrics handles it for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pybliometrics\n",
    "from pybliometrics.scopus import ScopusSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this workshop we are using my API key. This API key has been approved by Elsevier for use in the workshop. For security purposes, the API key is not written into this Jupyter Notebook. Please copy it from [this Google Doc](https://docs.google.com/document/d/1C1QfTnSnl6k0HVXfeD1xfjHnQgS8XUewW0Q5eeuxMQk/edit?usp=sharing). When you run the next cell, you will be prompted to enter in an API key. Paste in this key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pybliometrics.scopus.utils.create_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to import one last module before we can get working with the API data, but luckily this one came with Anaconda. **pandas** is a widely-adopted module for working with data in Python. It will help us organize the data that we get from different sources (such as the Scopus API) into a structured format, and will help us with organizing, manipulating, analyzing and visualizing the data.\n",
    "\n",
    "When we import pandas, we are going to give it the nickname `pd`; this is by convention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now ready to use the pybliometrics module to query the Scopus API. The only thing left to understand is how pybliometrics wants us to structure our commands. We can refer to the [module documentation](https://pybliometrics.readthedocs.io/en/stable/reference/pybliometrics.ScopusSearch.html) for this information, and the [Scopus API documentation](https://dev.elsevier.com/tips/ScopusSearchTips.htm) to know which fields we can search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by searching for articles where **Professor Aaron Fisk** is the **first author**. According to the Scopus API documentation, the field for first author is `FIRSTAUTH`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's find out how to construct our search using the ScopusSearch function\n",
    "?ScopusSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search the Scopus database for articles where Fisk A is the first author. 'FIRSTAUTH' is the field code for first author.\n",
    "s = ScopusSearch('FIRSTAUTH ( fisk a )')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# See how many results we got from the search.\n",
    "s.get_results_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame to store the results and assign it to variable 'df', that we just created\n",
    "df = pd.DataFrame(pd.DataFrame(s.results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrames â€” a quick aside\n",
    "We just created a DataFrame in pandas using the bibliographic data that we got from the Scopus API on our search. Let's take a closer look at what we did. A DataFrame is a 2-dimensional data structure (tabular) that can store data of different types in the columns. It is similar to a spreadsheet.\n",
    "\n",
    "DataFrames will become the base for all of our further analysis and work with data in Python. We'll describe DataFrames as we work along."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = ScopusSearch('AFFILORG (Great Lakes Institute for Environmental Research)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.get_results_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_glier = pd.DataFrame(pd.DataFrame(s.results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_glier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Challenge 6</b> \n",
    "</div>\n",
    "\n",
    "We've demonstrated searching for articles by searching for a particular first author, and by a particular affiliation. There are other ways to search as well.\n",
    "\n",
    "1. Consult the [Scopus API documentation](https://dev.elsevier.com/tips/ScopusSearchTips.htm) to find the field to search by Scopus **author ID**, the unique identifier that Scopus creates automatically for authors. \n",
    "2. Complete the API request below and search for Professor Aaron Fisk by his unique Scopus author ID: `7006248240`.\n",
    "3. Create a DataFrame out of the search results and assign it to `df_fisk`.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = ScopusSearch('______ (_________)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fisk = _______"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Searching by author name may bring in a number of irrelevant results. Citation indexes such as Scopus and Web of Science create [automatic profiles for authors](https://blog.scopus.com/posts/check-correct-submit-how-to-ensure-accuracy-in-your-scopus-author-profile). These profiles are created algorithmically by the databases using different data points to group articles by the author. While these author IDs aren't perfect, they are more precise than searching for an author's name by free-text.\n",
    "\n",
    "Let's construct a search where we find all the articles published by faculty members in the Department of Biological Sciences at the University of Windsor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by importing tabular data of these researchers and their Scopus ID. I have created this spreadsheet already by sourcing the author IDs in Scopus. We'll import this data by **reading the CSV** file into a pandas DataFrame. Earlier we created DataFrames from the data that we got from the Scopus API; reading data from a CSV is a much more common practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import list of researchers as a dataframe\n",
    "df_facultyMembers = pd.read_csv('data/researchers.csv')\n",
    "df_facultyMembers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list for author Scopus IDs\n",
    "authors = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populate list with Scopus IDs from the facutly member dataframe\n",
    "for scopusID in df_facultyMembers['scopusID']:\n",
    "    authors.append(scopusID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the query of Scopus IDs that will be used in the API call\n",
    "# Create an empty string\n",
    "query = ''\n",
    "# Add every Scopus ID to the string except for the last one, because the query command must not end with \"OR\"\n",
    "for author in authors[:-1]:\n",
    "    query = query + 'AU-ID(' + str(author) + ') OR '\n",
    "# Add the last Scopus ID  \n",
    "query = query + 'AU-ID(' + str(authors[-1]) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search the Scopus API for all publications by these authors, in one search\n",
    "s = ScopusSearch('{}'.format(query), refresh=True, subscriber=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the API results into a DataFrame\n",
    "df = pd.DataFrame(pd.DataFrame(s.results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Back-up\n",
    "If the API is not working, or if it is taking too long to get the results from the API, I have created a CSV with the results from the search above. If needed, uncomment (by getting rid of the `#`) the cell below and this will import the data into a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('data/backup-articles.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Data analysis and manipulation with pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the DataFrame\n",
    "Let's explore and describe the DataFrame that we've just created. \n",
    "\n",
    "As with any other object, we can use `type()` to determine the object type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This isn't surprising. Our object, `df` is a DataFrame. Let's now see what types the data within the DataFrame are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting. A few things to note. \n",
    "\n",
    "First, all the data within a column must be of the same type.\n",
    "\n",
    "Second, pandas uses different names for the data types than Python, though they refer to the same data type. In pandas:\n",
    "\n",
    "`object` is the same as `str`\n",
    "<br>\n",
    "`int64` is the same as `int`\n",
    "<br>\n",
    "`float64` is the same as `float`\n",
    "\n",
    "Third, the Scopus API returned a dataset where all the data is of the same type (`object` / `str`), even for data where that doesn't make sense. For example, the citedby_count refers to the number of times that article has been cited. This really should be an `int`, and we'll need to convert it so that we can use it for analysis later. Let's do that now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the citation data into integers\n",
    "df[\"citedby_count\"] = pd.to_numeric(df[\"citedby_count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the categorial open access data into integers\n",
    "df[\"openaccess\"] = pd.to_numeric(df[\"openaccess\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert author count data into integers\n",
    "df[\"author_count\"] = pd.to_numeric(df[\"author_count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of methods and functions that can help us describe the DataFrame. Let's try a few of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see all of the columns in the DataFrame\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine the shape of the DataFrame (number of rows, number of columns)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see the first 5 rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# see the last 5 rows\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is even a method that automatically generates descriptive statistics on all available columns with numbers. Use `.describe()` to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the DataFrame columns\n",
    "\n",
    "In addition to the methods and functions for describing the DataFrames, there are a useful tools for describing **columns** within a DataFrame.\n",
    "\n",
    "We can call these methods by first **subsetting** the column of interest. We do this in a similar fashion as we did with lists â€“ we use the square brackets along with the column name. For example, if we wanted to subset the author_count column, we would do so like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['author_count']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This perhaps isn't as useful on its own. There are a number of methods for pandas columns which we can use to help us understand the data in the column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['author_count'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['author_count'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['author_count'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['author_count'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['author_count'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['author_count'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['fund_sponsor'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['fund_sponsor'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['fund_sponsor'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Challenge 7</b> \n",
    "</div>\n",
    "\n",
    "Can you identify what the top three journals published by these researchers are?\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing and slicing the pandas DataFrame\n",
    "Sometimes we don't need to work with the entre DataFrame, but only some elements of it.\n",
    "\n",
    "We saw earlier that we can select a particular column by using square brackets and writing the name of the column. We can do this for multiple columns at once by passing a list of column names in the square brackets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['title', 'author_names', 'fund_sponsor']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps, what is more useful is subsetting data from the columns using criteria; this is called slicing and selecting. This looks a bit different than what we're used to in regular Python.\n",
    "\n",
    "As an example, let's only find those articles that have been published after 2019. Recall that there is a column named 'coverDate' that refers to the date the article was published."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['coverDate'] > '2019']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the basic operators that we learned about earlier, we can have pandas select only the articles that meet our criteria.\n",
    "\n",
    "We can also have pandas slice and select using **multiple** criteria. Note that we need to wrap each section in parantheses, and combine the two of them with the `&` or `|` operators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df['coverDate'] > '2019') & (df['publicationName'] == 'Scientific Reports')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find the articles that have been received more than 100 citations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['citedby_count'] > 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[df['citedby_count'] > 100].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a DataFrame for just journals\n",
    "We're really only interested in the journals at the moment, and not the individual articles. Let's create a new DataFrame that contains just the unique instances of journals and the times that they were published in by these researchers.\n",
    "\n",
    "We can get started by **grouping** the article records by the 'source_id', a unique identifier that Scopus gives for publications, counting the number of times a publication has been published in, and creating a new column for that variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['count'] = df.groupby('source_id')['source_id'].transform('count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a new column next to each article, that indicates how many times that journal has been published in by these researchers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a new DataFrame to store this information, and fix it up a bit so that it only contains the information that we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataDrame just for the journals, and remove any row that contains a duplicate of the source_id\n",
    "df_journals = df.drop_duplicates('source_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this new DataFrame, select only the columns that we need to describe the journal\n",
    "df_journals = df_journals[['publicationName','source_id', 'issn', 'eIssn', 'aggregationType', 'count']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the dataframe by publication count\n",
    "df_journals = df_journals.sort_values(by=['count'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the index count for this dataframe\n",
    "df_journals = df_journals.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VoilÃ !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_journals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a closer look at this new DataFrame. What stands out to you? \n",
    "\n",
    "There are some rows where the publication type is not a journal. We can see some conference proceedings, and some book series. Let's double check that by running the `value_counts()` method on the `aggregationType` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['aggregationType'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've confirmed that there are other publication types in this dataset. For our analysis, we want to stick to just journals. Let's get rid of the rows that aren't journals. Let's start by slicing the DataFrame and finding everything that **is not** a journal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_journals[df_journals['aggregationType'] != 'Journal']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can remove the rows that we don't want using the `.drop()` method, but only if we know the indexes of the rows that we don't want. We can use the `.index` method to find these indexes from the slice that we created. Let's store them in a new variable called indexNames, which we will then pass to `.drop()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexNames = df_journals[df_journals['aggregationType'] != 'Journal'].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the indexes of the rows that we don't want stored in indexNames, let's pass this variable as an argument for the `.drop()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_journals.drop(indexNames, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_journals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also some journals that have only an ISSN, or only an eISSN. For our work with the link-resolver later, we'll need to have at least one of these. Let's double check for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the index count for this dataframe\n",
    "df_journals = df_journals.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Challenge 8</b> \n",
    "</div>\n",
    "\n",
    "Using the same process that we used above, can you slice this DataFrame to find all the journals that have a missing value for **either** the `issn` or the `eIssn` columns?\n",
    "\n",
    "Are there any rows that are missing **both** the `issn` and the `eIssn` data?\n",
    "\n",
    "**Hint**: instead of using a comparison operator (`==`, `!=`), there is a special *method* in pandas to find the cells with missing values. This method is called [`.isnull()`](https://pandas.pydata.org/pandas-docs/version/0.23.4/generated/pandas.isnull.html).\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to need to have one column which has at least the `issn` or the `eIssn`, so that we can pass something along to the link resolver to check.\n",
    "\n",
    "Let's create a new column, `q_issn`, for the issn that we'll use in the query. We'll duplicate the values from the `eIssn` column in this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a column with an ISSN for searching, and populate with the values for the eISSN\n",
    "df_journals['q_issn'] = df_journals['eIssn']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not all rows will have an `eIssn`. For those rows, we'll place the regular `issn` in the `q_issn` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For those rows where there is no eISSN, replace them with the ISSN\n",
    "df_journals.loc[df_journals['q_issn'].isnull(),'q_issn'] = df_journals['issn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_journals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One last thing. There are 395 rows in this DataFrame but the index numbers remain from before we did the final cleanup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a cleaned-up DataFrame of journals that we can use with the link resolver. This might be useful for our purposes, since this gives us a look at what publications our researchers publish in and how frequently. Let's export this from Python and keep it as a CSV for further use later on.\n",
    "\n",
    "pandas has a method for this purpose called `.to_csv()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_journals.to_csv('data_output/popular-biology-journals.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find what journals our researchers cited\n",
    "In addition to acting as a bibliographic database and providing us with data on the articles that our researchers published, Scopus, and other citation indexes, provide citation data. These databases provide information both on the references in these articles, and the who has cited these articles.\n",
    "\n",
    "For collections purposes, understanding what journals our researchers are reading is an important metric. Elsevier has a seperate API, the [Scopus Abstract Retrieval API](https://dev.elsevier.com/documentation/AbstractRetrievalAPI.wadl) that will provide us with data on the references in the bibliographic records of our researchers articles.\n",
    "\n",
    "To get started, we'll need to import another pybliometrics function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pybliometrics.scopus import AbstractRetrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the [pybliometrics documentation for this function](https://pybliometrics.readthedocs.io/en/stable/reference/pybliometrics.AbstractRetrieval.html), we must provide the function with a unique identifer for the article (the Scoups EID, the Scopus ID, the PII, the Pubmed-ID or the DOI).\n",
    "\n",
    "Looking at the `df` DataFrame that contains 1,400 + references by our researchers, the Scopus EID seems to be the most complete datafield, so we'll go with this.\n",
    "\n",
    "Let's try out the AbstractRetrieval function on one article from the `df` DataFrame: `2-s2.0-85073025265`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "abstract = AbstractRetrieval('2-s2.0-85073025265', view='FULL')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `.references` method to see all of the references."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract.references"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can read it easier if we put it into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_references = pd.DataFrame(abstract.references)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the references for just one article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_references"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This could be useful information, but to get it for all of the articles, we would need to do this 1,400 + times!!! That is way too much to do by hand. Luckily, we can use Python to create a function to auotomate this. The user-defined function below, `getReferences`, will craft an API request for every `eid` in the DataFrame of researcher articles `df`, and store these references in new DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getReferences(row):\n",
    "    ab = AbstractRetrieval(row['eid'], view='FULL')\n",
    "    df_temp = pd.DataFrame(ab.references)\n",
    "    df_temp['citingArticle'] = row['eid']\n",
    "    global df_refs\n",
    "    df_refs = pd.concat([df_refs, df_temp], sort=False)\n",
    "    print('References retrieved for article \\\"' + row['eid'] + '\\\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create an empty DataFrame to store the references that we collect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_refs = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now apply our user-defined function using the method `.apply`. **Note**: for the purposes of demonstration, I will only do this on the first 10 of the 1,400 articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Search the Open URL link resolver to find the coverage for these journals\n",
    "df[0:10].apply(getReferences,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the articles that our researchers referenced in their publications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_refs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's undertake the same process as before to identify the journals in this dataset, remove duplicates, count the number of times the journals are referenced and sort it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_refs['count'] = df_refs.groupby('sourcetitle')['sourcetitle'].transform('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe of journals, with duplicates of journals removed\n",
    "df_journalsReferenced = df_refs.drop_duplicates('sourcetitle')\n",
    "\n",
    "# Sort the dataframe by publication count\n",
    "df_journalsReferenced = df_journalsReferenced.sort_values(by=['count'], ascending=False)\n",
    "\n",
    "# Reset the index count for this dataframe\n",
    "df_journalsReferenced = df_journalsReferenced.reset_index(drop=True)\n",
    "\n",
    "# Get rid of unnecessary columns\n",
    "df_journalsReferenced = df_journalsReferenced[['sourcetitle', 'count']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_journalsReferenced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify journal holdings with a library link resolver\n",
    "In this final section of the workshop, we'll take a look at how we can automate the process of checking to see if the library actually has access to the journals that our journals are publishing in, or the journals they are reading.\n",
    "\n",
    "#### What do library link resolvers do and how do they work?\n",
    "A library link resolver exists to help a user identify if a library's collection contains a certain resource. The most common use case is in the form of a 'Get-It' button that appears in the search results of a bibliographic database.\n",
    "\n",
    "This is a UWindsor link-resolver link for an article record that was found in PubMed.\n",
    "\n",
    "[<img src=\"img/get-it.gif\"/>](http://primo.uwindsor.ca/openurl/UWINDSOR/UWINDSOR_SERVICES?sid=Entrez:PubMed&id=pmid:31593066)\n",
    "\n",
    "Clicking on the 'Get-it' button takes you to the link resolver, but the database sends a bunch of information about the resource along with you. Take a look at the URL and see if you can recognize some of the *metadata* ;) In this case we can find a unique identifier for the article: the PubMed identifier. This is enough information for the University of Windsor integrated library system (Alma) to check and see if we have access to the journal this article is published in.\n",
    "\n",
    "The format of the information that a bibliographic database sends, and that a link-resolver uses, is not institution or product-specific. The metadata is formatted according to the [**OpenURL specification**](https://en.wikipedia.org/wiki/OpenURL), which is widely adopted by database providers and link resolvers. The good news is that the following process can apply to almost every one of your institutions!\n",
    "\n",
    "#### How can we get data from them?\n",
    "Behind the graphical interfaces, a link resolver operates very much like the APIs that we've been working with. Like an API, the link resolver requests are sent over the internet as an HTTP request. Like an API, the link resolver returns machine-readable data in the form of a XML or JSON file (though it is parsed out and presented nicely to users). \n",
    "\n",
    "Here is an example of a link resolver request for the UWindsor Alma link resolver. This request is specfically to resolve a journal. The request is formatted according to the OpenURL specification. \n",
    "\n",
    "[`http://na01.alma.exlibrisgroup.com/view/uresolver/01UTON_UW/openurl?svc_dat=CTO&issn=03801330`](http://na01.alma.exlibrisgroup.com/view/uresolver/01UTON_UW/openurl?svc_dat=CTO&issn=03801330)\n",
    "\n",
    "Let's deconstruct this URL request.\n",
    "\n",
    "> `http://na01.alma.exlibrisgroup.com/view/uresolver/01UTON_UW/openurl`: the base URL for the UWindsor Alma link resolver. Obviously, this will vary by institution.\n",
    "\n",
    "> `?`: the start of the query\n",
    "\n",
    "> `svc_dat=CTO`: metadata indicating that the response from the link resolver should be in XML format.\n",
    "\n",
    "> `&`: the operator to combine arguments in the URL request\n",
    "\n",
    "> `issn=03801330`: the query itself; simply just passing along an ISSN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Challenge 9</b> \n",
    "</div>\n",
    "\n",
    "Let's get familiar with the data that we get from an OpenURL link resolver. Using all the parts of an OpenURL request, as described above:\n",
    "\n",
    "1. Create an OpenURL query to see if the University of Windsor subscribes to. From what providers and for what coverage dates?\n",
    "    * *Environmental Science and Technology*, ISSN: 0013936X\n",
    "    * *Journal of Fish Biology*, ISSN: 00221112\n",
    "    * *Nature Sustainability*, ISSN: 23989629\n",
    "    \n",
    "    \n",
    "2. Take some time to read through the XML output. What is the pattern? What kind of data is returned and how is it structured?\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like the Scopus API request earlier, let's try to make the same HTTP request in Python using the `requests` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the requests module send our link resolver request, and then save it to the variable 'r'\n",
    "r = requests.get('http://na01.alma.exlibrisgroup.com/view/uresolver/01UTON_UW/openurl?svc_dat=CTO&issn=03801330')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "r.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not pretty, but its all there.\n",
    "\n",
    "Unfortunately, unlike for Scopus, a Python wrapper for the link resolver does not exist. This means that **we** will have to develop our own code and functions to make these requests, get the responses, and parse out the relevant information from the XML. \n",
    "\n",
    "I developed the functions below to do this job. They may look a bit overwhelming at first, but look to see if you can notice some patterns in the way the function is written. \n",
    "\n",
    "First, let's import the necessary modules for this code to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ElementTree\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xml.etree.ElementTree is a module for working with XML data. It contains functions to parse out the information in the XML response.\n",
    "\n",
    "re is a module for working with characters. It works to parse out text from the XML response.\n",
    "\n",
    "We are now ready to write our user user-defined definitions. \n",
    "\n",
    "The first one, `getText` gets the text from the XML response. This function is used *inside* the second function that we're defining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getText(elem):\n",
    "    ''' \n",
    "    Get the text from the XML response and return a string.\n",
    "    '''\n",
    "    try:\n",
    "        msg = elem.text  \n",
    "        msg = msg.replace('<br>', '')\n",
    "    except:\n",
    "        msg = \"\"\n",
    "\n",
    "    if msg is None:\n",
    "        msg = \"not available\"\n",
    "\n",
    "    return msg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second function, `searchOpenURL` creates an Open URL request from the ISSN of journals stored in a DataFrame. We will apply this to a DataFrame of journals with ISSNs that we want to check against a link resolver to see if the library has it.\n",
    "\n",
    "Take a look at the comments in the function to see what does what."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def searchOpenURL(row):\n",
    "    '''(pandas.DataFrame) --> pandas.Series\n",
    "    \n",
    "    This function takes a row of a pandas DataFrame and gets the ISSN of a journal. When used with the pandas apply function, this function uses the ISSNs to run a HTTP query against a library OpenURL link resolver server, retrieves the XML response, and parses out the package name and coverage dates. Using getText, this function returns two columns in a panda Series for each row: a statement of availability, and a statement of coverage (package names and the dates they cover).\n",
    "    \n",
    "    '''\n",
    "    # Create and run an HTTP request against the open URL link resolver \n",
    "    r = requests.get('http://na01.alma.exlibrisgroup.com/view/uresolver/01UTON_UW/openurl?svc_dat=CTO&issn={}'.format(row['q_issn']))\n",
    "    # Parse the XML response and store it as root\n",
    "    root = ElementTree.fromstring(r.content)\n",
    "    # Create a dict of namespace values for use later on, so that the queries of the stored XML response can be cleaner\n",
    "    ns = {'resolver': 'http://com/exlibris/urm/uresolver/xmlbeans/u'}\n",
    "    # Create an empty dict that will be used to store the coverage statements for each journal. Key will be the package name, value will be the coverage dates.\n",
    "    coverage_statement = {}\n",
    "    \n",
    "    # get all full-text services\n",
    "    \n",
    "    # if there is a full-text service\n",
    "    if root.findall('.//resolver:context_service[@service_type=\"getFullTxt\"]',ns) != []:\n",
    "        # set the availability statement to show that there is a full-text\n",
    "        avail_statement = 'Full-text available'\n",
    "        print('Full-text available for ' + row['q_issn'])\n",
    "        # for each full-text service\n",
    "        for service in root.findall('.//resolver:context_service[@service_type=\"getFullTxt\"]',ns):\n",
    "            # Create empty str variables to store the details of the full-text service\n",
    "            servicePackageName = ''\n",
    "            serviceCoverage = ''\n",
    "            # get package name of the full-text service and add it to the temporary str variable\n",
    "            package = service.find('.//resolver:key[@id=\"package_public_name\"]',ns)\n",
    "            servicePackageName = getText(package)\n",
    "            \n",
    "            # get coverage date statement of the full-text service and add it to the temptorary str variable\n",
    "            avail = service.find('.//resolver:key[@id=\"Availability\"]',ns)\n",
    "            serviceCoverage = getText(avail)\n",
    "            # Add the details of this full-text service to the dict\n",
    "            coverage_statement[servicePackageName] = serviceCoverage\n",
    "    # When there is no full-text service\n",
    "    else:\n",
    "        # set the availability statement to show that there is no full-text\n",
    "        avail_statement = 'No full-text available'\n",
    "        print('Full-text not available for ' + row['q_issn'])\n",
    "    # Return the availability and coverage statements as a pandas Series\n",
    "    return pd.Series([avail_statement, coverage_statement])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we just defined the function that will automatically carry out the process of checking the link resolver for a journal ISSN, we can now apply the function to the DataFrame with the journals that our researchers publish in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search the Open URL link resolver to find the coverage for these journals\n",
    "df_journals[['availability', 'coverage']] = df_journals.apply(searchOpenURL ,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_journals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neat! This has checked every journal in the df_journals DataFrame (the DataFrame with the journals that our research publish in), and has added two columns: one that contains a statement as to whether or not we have access to the full-text, and another column that contains information on the vendors.\n",
    "\n",
    "Let's see how the coverage data looks by querying what is in that cell for the first row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_journals.iloc[0]['coverage']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_journals.iloc[17]['coverage']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_journals.iloc[26]['coverage']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_journals.iloc[392]['coverage']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the full-text coverage for journals is not equal across all journals. For the journal at index 0 of the DataFrame (*Journal of Great Lakes Research*), we have access from three vendors, resulting in access from 1975 to the present. \n",
    "\n",
    "For the second journal at index 17 of the DataFrame (*Proceedings of the Royal Society B: Biological Sciences*), we have access to the journal from three vendors, but access to the most recent material is embargoed by 1 year.\n",
    "\n",
    "For the third journal at index 26 of the DataFrame (*Marine Ecology Progress Series*), we only have access from a particular date range. The journal is not embargoed, our coverage just stops.\n",
    "\n",
    "For the fourth journal at index 392 (*Iranian Journal of Fisheries Sciences*), we simply don't have any coverage at all.\n",
    "\n",
    "The statements that we created in the availability column therefore are clear for every journal. We may have 'full-text' access for a these journals, but is it access to the present, access that is embargoed or access that ended a few years ago? There is no metadata from the link resolver on this, but we can spot patterns in the way that the coverage statements were written. We can take advantage of these patterns by parsing out this text and creating a new statement for each journal that expresses what form of full-text access we have. \n",
    "\n",
    "This calls for a new user-defined function; again, take a look at the comments to see how this function was created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coverageStatement_availParser(row):\n",
    "    '''\n",
    "    (pd.Series) -> pd.Series\n",
    "    \n",
    "    This function parses out info from the coverage statements for all packages, and updates the availability statements for the journals to reflect those journals that don't have full-text coverage, those that do up to the present, those with embargo and those with full-text access, but not to the present.\n",
    "    \n",
    "    '''\n",
    "    # Create an empy str for the coverage statement value\n",
    "    avail_statement = ''\n",
    "    # Only do run this function if there are full-text resources\n",
    "    if row['coverage'] != {}:\n",
    "        # Create an empty variable that will change if the function should stop\n",
    "        stop = 0\n",
    "        # Check all coverage statements in the dict, and if any ONE of them doesn't contain the words 'most recent' or 'until' (i.e., its up to the current), set the availability statement to available to present and stop.\n",
    "        for value in row['coverage'].values():\n",
    "            # Skip values that don't contain any data\n",
    "            if value != '':\n",
    "                if not any(s in value for s in ('Most recent', 'until')):\n",
    "                    avail_statement = 'Full-text available to present'\n",
    "                    stop = 1\n",
    "                    break\n",
    "        # If there was no coverage statement where there was full-text to the present, continue\n",
    "        if stop == 0:\n",
    "            for value in row['coverage'].values():\n",
    "                if value != '':\n",
    "                    # If there is any ONE line coverage statement that is for an embargo\n",
    "                    if 'Most recent' in value:\n",
    "                        avail_statement = 'Full-text available with embargo'\n",
    "                        stop = 1\n",
    "                        break\n",
    "        # If there is no statement up to the present, nor for an embargo, then it must be available, but not complete.\n",
    "        if stop == 0:\n",
    "            for value in row['coverage'].values():\n",
    "                if value != '':\n",
    "                    if 'until' in value:\n",
    "                        avail_statement = 'Full-text available, but not complete'\n",
    "    else:\n",
    "        avail_statement = 'No full-text available'\n",
    "    return pd.Series([avail_statement])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now apply this function to all the rows in the DataFrame. Because we aren't using the internet for this, Python will quickly execute this operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the availability statements based on the coverage dates (emabargo, not to the present)\n",
    "df_journals[['availability']] = df_journals.apply(coverageStatement_availParser,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_journals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each journal now has a new availability statement in the `availability` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_journals['availability'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Challenge 10</b> \n",
    "</div>\n",
    "\n",
    "Now that there are four forms of availabilty statements, we can sort the DataFrame according to these statements. [Recall from earlier: how indexing and slicing works in pandas](#Indexing-and-slicing-the-pandas-DataFrame).\n",
    "\n",
    "1. Slice and select only those rows in the df_journals DataFrame that have the `Full-text available with embargo` availability statement.\n",
    "\n",
    "2. Slice and select only those rows in the df_journals DataFrame that have the `No full-text available` availability statement.\n",
    "\n",
    "3. Slice and select only those rows in the df_journals DataFrame that have the `Full-text available, but not complete` availability statement.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Challenge 11</b> \n",
    "</div>\n",
    "\n",
    "Using the link resolver isn't limited to only journals that our researchers have published in â€“ in fact, with the functions that we created, we can have the link resolver automatically check any ISSN. \n",
    "\n",
    "In the data folder of this directory you will find a CSV file that contains journals ranked in the [Scimago Journal Rankings (SJR) organic chemistry](https://www.scimagojr.com/journalrank.php?area=1600&category=1605) category. This CSV file contains lots of information on the metrics of the journal, but we're just interested in the SJR rank and the ISSN number (so we can check to see if the library subscribes to the journal.\n",
    "\n",
    "For this workshop, I've already created a `q_issn` column that contains the ISSN for the journal, so it can easily integrate with the functions that we created. \n",
    "\n",
    "1. Import the CSV into a new pandas DataFrame, and give it the name `df_organic`.\n",
    "\n",
    "2. Use the `searchOpenURL` function we created to check and see if we have the journals.\n",
    "\n",
    "3. Use the `coverageStatement` function we created to update the coverage statements.\n",
    "\n",
    "4. Get the value counts for the coverage statements (like above), and slice and select the journals according to the availability statements.\n",
    "\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Practice on your own</b> Time permitting, use the Jupyter Notebook in part III to create a Scopus API request of your own, create a DataFrame to keep this data, clean-up, organize and manipulate the dataset in pandas, and run link resolver queries with the UWindsor OpenURL link resolver.\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
